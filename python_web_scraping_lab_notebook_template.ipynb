{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXQLb9dAadRFApIBtJXL8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwaldenphd/web-scraping-python/blob/main/python_web_scraping_lab_notebook_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping in Python "
      ],
      "metadata": {
        "id": "7MzhNm9Tt6d3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff876fb9"
      },
      "source": [
        "Student Name: **Enter Your Name Here (Double click to edit)**\n",
        "<br>\n",
        "Net ID: **Enter Your NetID Here (Double click to edit)**\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Notebook Questions"
      ],
      "metadata": {
        "id": "tQARbhvayCrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1: Describe the general approach to loading a web page in Python using `requests` and isolating the section of HTML you need using `BeautifulSoup`. What are the basic steps involved in this workflow, thinking about what happens at the start of the program to isolate the section of HTML you would need to do further work with to extract the data you want to work with?**\n"
      ],
      "metadata": {
        "id": "pgEKP1FtpExo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0cFnREpis-5"
      },
      "source": [
        "**AnswerQ1**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your codes here (double click to edit)"
      ],
      "metadata": {
        "id": "3ljNdsyxpQRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2A: Select another Wikipedia page that includes a table. From looking at the public web page, what data do you want to scrape from this web page (i.e. specific table, multiple tables, etc.)? What do you want the resulting data structure to look like (columns, rows, etc)?**\n",
        "\n",
        "### **Q2B: Take a look at the HTML for this page. What tags or other HTML components do you see around the section of the page you want to work with? For this question, we're thinking about how we will end up writing a program with <code>BeautifulSoup</code> to isolate a section of the web page.**\n",
        "\n",
        "### **Q2C: Develop an outline for a Python program that scrapes data from the web page you selected.**\n",
        "\n",
        "A preliminary workflow:\n",
        "- Load URL and create BeautifulSoup object\n",
        "- Isolate section of HTML with your table (either directly or extract from list)\n",
        "- Isolate table row elements (create list where each element is a table row)\n",
        "- Extract contents from row (isolate the pieces of information from each row)\n",
        "- Create Pandas DataFrame\n",
        "- Write extracted row contents to CSV file\n",
        "\n",
        "NOTE: You do not need to have working code for all components of this program. That's where we're heading with the final project. At this point, we're focusing on the conceptual framework for the web scraping program. Start to build out code where you can, but think about the programming version of outlining a paper.\n",
        "\n",
        "### **Q2D: What challenges or roadblocks did you face working on Q2C? What parts of the program do you understand/feel ready to develop at this point? What parts of the program are less clear?**"
      ],
      "metadata": {
        "id": "FembQg4kp_M2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2XYT06SpfJQ"
      },
      "source": [
        "**AnswerQ2**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your codes here (double click to edit)\n",
        "\n",
        "# load url & create beautiful soup object\n",
        "\n",
        "# isolate page section\n",
        "\n",
        "# create list of rows"
      ],
      "metadata": {
        "id": "LWk_FYvnuw1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on single row\n",
        "\n",
        "# extract contents from row"
      ],
      "metadata": {
        "id": "Y9M-9WXkJSoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over rows in table\n",
        "\n",
        "# create pandas dataframe\n",
        "\n",
        "# write to csv file"
      ],
      "metadata": {
        "id": "Abi0xNTZJU28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3: Describe in your own words how the url generation program covered in the previous section of the lab works. The full program is also included below. What is happening in the different program components?** "
      ],
      "metadata": {
        "id": "3URjcSZDpbFu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB-kFikxis_I"
      },
      "source": [
        "**AnswerQ3:** Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeLFq06ku83j"
      },
      "outputs": [],
      "source": [
        "# your codes here (double click to edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q4: Select another Sports Reference web page that follows this pattern and write a program that generates a list of full URLs for that team/organization.**\n",
        "\n",
        "A few places to start:\n",
        "- Baseball Reference season web pages have the following URL pattern:\n",
        "  * `https://www.baseball-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.shtml`\n",
        "- Basketball Reference season web pages have a similar pattern for NBA teams:\n",
        "  * `https://www.basketball-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.html`\n",
        "- Basketball Reference uses a slightly different pattern for its WNBA pages:\n",
        "  * `https://www.basketball-reference.com/wnba/teams`, `TEAM ABBREVIATION`, `SEASON`, `.html`\n",
        "- College Basketball Reference pages also follow a pattern: \n",
        "  * `https://www.sports-reference.com/cbb/schools`, `SCHOOL ABBREVIATION`, `SEASON`, `.html`\n",
        "- For Hockey Reference pages: \n",
        "  * `https://www.hockey-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.html`\n",
        "- Football Reference pages follow the same pattern for men's and women's teams:\n",
        "  * `https://fbref.com/en/squads/`, `SQUAD ID`, `SEASON`, `TEAM NAME`\n",
        "- Pro Football Reference pages also have a pattern: \n",
        "  * `https://www.pro-football-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.htm`\n",
        "\n",
        "NOTE: You DO NOT need to write a program that scrapes data from these pages for this question. The purpose of this question is to be able to programmatically generate a list of URLs that cover a date range. \n"
      ],
      "metadata": {
        "id": "1HWIBpTsawd8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf3iN_D7is_N"
      },
      "source": [
        "**AnswerQ4:** Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jRJxH98azdc"
      },
      "outputs": [],
      "source": [
        "# your codes here (double click to edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q5: Describe the general approach to loading a web page in Python using `pd.read_html()`. What are the basic steps involved in this workflow, thinking about what happens to identify/isolate the specific table you want to work with?**"
      ],
      "metadata": {
        "id": "NIornfjZpmFy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BAA-_Vyis_X"
      },
      "source": [
        "**AnswerQ5:** Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_wK6Djjd3TQ"
      },
      "outputs": [],
      "source": [
        "# your codes here (double click to edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q6: For Q4, you generated a list of Sports Reference URLs covering a time span for a specific team/organization. Select three years and web pages from that list- something early in the time period covered, something in the middle of the time period covered, and something toward the end of the time period covered. Do these pages have the same pattern in terms of number and order of tables? For one of these pages, what table or tables on these pages would you want to be able to extract and work with?**"
      ],
      "metadata": {
        "id": "ymUHiLayptBD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcRpdIN_is_b"
      },
      "source": [
        "**AnswerQ6:** Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lT7urowd9w3"
      },
      "outputs": [],
      "source": [
        "# your codes here (double click to edit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q7A: Develop an outline for a Python program that uses `pd.read_html()` to scrape data from one of the web pages you select in Q6.**\n",
        "\n",
        "A preliminary workflow:\n",
        "- Use `pd.read_html()` to create a list of DataFrame objects\n",
        "- Identify which DataFrame object in the list is the table you want to work with\n",
        "- Isolate the list element to create a new DataFrame\n",
        "- Write the new DataFrame to a CSV file\n",
        "\n",
        "NOTE: For Q3, you did not need to have working code for all components of this program. Since `pd.read_html()` has an easier learning curve, let's see if we can flesh out more of this program. But if you run into problems, it's okay to focus on the conceptual framework for the web scraping program. Start to build out code where you can, but think about the programming version of outlining a paper.\n",
        "\n",
        "ANOTHER NOTE: For many Sports Reference pages, tables further down the page are buried in HTML comments. These tables will not show up when you use `pd.read_html()`. We can come back to these \"hidden tables\" in the final project, but for now, focus on the tables that do show up when you use `pd.read_html()`.\n",
        "\n",
        "### **Q7B: What challenges or roadblocks did you face working on Q7A? What parts of the program do you understand and/or were able to develop? What parts of the program are less clear?**"
      ],
      "metadata": {
        "id": "HtUKT1xNqJQy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tihNYy4uis_c"
      },
      "source": [
        "**AnswerQ7:** Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roxNuL4SHkw3"
      },
      "outputs": [],
      "source": [
        "# your codes here (double click to edit)\n",
        "\n",
        "# create list of dataframes \n",
        "\n",
        "# isolate specific dataframe\n",
        "\n",
        "# write dataframe to csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q8A: Select another web page that includes unstructured text. From looking at the public web page, what text do you want to scrape from this web page (i.e. specific sections, multiple paragraphs, etc.)?**\n",
        "\n",
        "A few places to start for unstructured text:\n",
        "- [The Observer!](https://ndsmcobserver.com) (or another news publication of your choosing)\n",
        "- [WikiSource](https://en.wikisource.org/wiki/Main_Page)(a library of texts that are not covered by copyright)\n",
        "  * [U.S. Presidential State of the Union Addresses](https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents)\n",
        "  * [U.S. Presidential Inaugural Speeches](https://en.wikisource.org/wiki/Portal:Inaugural_Speeches_by_United_States_Presidents)\n",
        "- [Project Gutenberg](https://www.gutenberg.org) (a library of literary works or texts that are not covered by copyright)\n",
        "\n",
        "### **Q8B: Take a look at the HTML for this page. What tags or other HTML components do you see around the section of the page you want to work with? For this question, we're thinking about how we will end up writing a program with `BeautifulSoup` to isolate a section of the web page.**\n",
        "\n",
        "### **Q8C: Develop an outline for a Python program that scrapes unstructured text from the web page you selected.**\n",
        "\n",
        "A preliminary workflow:\n",
        "- Load URL and create BeautifulSoup object\n",
        "- Isolate section of HTML with your text (either directly or extract from list)\n",
        "- IF NEEDED: Isolate text elements (create list where each element is a section of text)\n",
        "- IF NEEDED: Extract text contents (isolate text from each section/paragraph)\n",
        "- Write text to TXT file\n",
        "\n",
        "NOTE: You do not need to have working code for all components of this program. That's where we're heading with the final project. At this point, we're focusing on the conceptual framework for the web scraping program. Start to build out code where you can, but think about the programming version of outlining a paper.\n",
        "\n",
        "### **Q8D: What challenges or roadblocks did you face working on Q8C? What parts of the program do you understand/feel ready to develop at this point? What parts of the program are less clear?**"
      ],
      "metadata": {
        "id": "vu71MpmSivh5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4XagE0Va57Q"
      },
      "source": [
        "**AnswerQ8:** Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f8fxymXICRJ"
      },
      "outputs": [],
      "source": [
        "# your codes here (double click to edit)\n",
        "\n",
        "# load url & create beautiful soup object\n",
        "\n",
        "# isolate section of page with text\n",
        "\n",
        "# isolate text elements & extract text contents\n",
        "\n",
        "# write to TXT file"
      ]
    }
  ]
}