{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwaldenphd/web-scraping-python/blob/main/python_web_scraping_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a1c5473",
      "metadata": {
        "id": "8a1c5473"
      },
      "source": [
        "# Web Scraping in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff876fb9",
      "metadata": {
        "id": "ff876fb9"
      },
      "source": [
        "Student Name: **Enter Your Name Here (Double click to edit)**\n",
        "<br>\n",
        "Net ID: **Enter Your NetID Here (Double click to edit)**\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad74de10",
      "metadata": {
        "id": "ad74de10"
      },
      "source": [
        "NOTE: Remember to submit two versions of your lab notebook on Canvas- a Jupyter Notebook (`.ipynb`) file and a PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Notebook Questions"
      ],
      "metadata": {
        "id": "qPNYse_HPJ1Y"
      },
      "id": "qPNYse_HPJ1Y"
    },
    {
      "cell_type": "markdown",
      "id": "34749159",
      "metadata": {
        "id": "34749159"
      },
      "source": [
        "### **Q1: Describe the general approach to loading a web page in Python using `requests` and isolating the section of HTML you need using `BeautifulSoup`. What are the basic steps involved in this workflow, thinking about what happens at the start of the program to isolate the section of HTML you would need to do further work with to extract the data you want to work with?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b239bb6",
      "metadata": {
        "id": "3b239bb6"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c1abb3",
      "metadata": {
        "id": "83c1abb3"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e59cb7cd",
      "metadata": {
        "id": "e59cb7cd"
      },
      "source": [
        "### **Q2: Select another Wikipedia page that includes a table. From looking at the public web page, what data do you want to scrape from this web page (i.e. specific table, multiple tables, etc.)? What do you want the resulting data structure to look like (columns, rows, etc)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312b60ae",
      "metadata": {
        "id": "312b60ae"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dcd7595",
      "metadata": {
        "id": "1dcd7595"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d5cb9e",
      "metadata": {
        "id": "59d5cb9e"
      },
      "source": [
        "### **Q3: Take a look at the HTML for this page. What tags or other HTML components do you see around the section of the page you want to work with? For this question, we're thinking about how we will end up writing a program with <code>BeautifulSoup</code> to isolate a section of the web page.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62179eb3",
      "metadata": {
        "id": "62179eb3"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e18b411",
      "metadata": {
        "id": "6e18b411"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d992192",
      "metadata": {
        "id": "3d992192"
      },
      "source": [
        "### <strong>Q4: Develop an outline for a Python program that scrapes data from the web page you selected. \n",
        "\n",
        "A preliminary workflow:\n",
        "- Load URL and create BeautifulSoup object\n",
        "- Isolate section of HTML with your table (either directly or extract from list)\n",
        "- Isolate table row elements (create list where each element is a table row)\n",
        "- Extract contents from row (isolate the pieces of information from each row)\n",
        "- Write extracted row contents to CSV file\n",
        "\n",
        "NOTE: You do not need to have working code for all components of this program. That's where we're heading with the final project. At this point, we're focusing on the conceptual framework for the web scraping program. Start to build out code where you can, but think about the programming version of outlining a paper.</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad1b7f77",
      "metadata": {
        "id": "ad1b7f77"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c5158e",
      "metadata": {
        "id": "80c5158e"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da54b70",
      "metadata": {
        "id": "3da54b70"
      },
      "source": [
        "### **Q5: What challenges or roadblocks did you face working on Q4? What parts of the program do you understand/feel ready to develop at this point? What parts of the program are less clear?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbd4170",
      "metadata": {
        "id": "afbd4170"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2586e5a",
      "metadata": {
        "id": "a2586e5a"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78d000a",
      "metadata": {
        "id": "b78d000a"
      },
      "source": [
        "### **Q6: Describe in your own words how the url generation program covered in the previous section of the lab works. The full program is also included below. What is happening in the different program components?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26355eba",
      "metadata": {
        "id": "26355eba"
      },
      "outputs": [],
      "source": [
        "root = \"https://www.sports-reference.com/cfb/schools/notre-dame/\"\n",
        "\n",
        "years = range(1899, 2021, 1)\n",
        "\n",
        "tag = \"-schedule.html\"\n",
        "\n",
        "urls = []\n",
        "\n",
        "for year in years:\n",
        "    urls.append(root + str(year) + tag)\n",
        "    \n",
        "urls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc10cba9",
      "metadata": {
        "id": "bc10cba9"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f3e65d",
      "metadata": {
        "id": "b3f3e65d"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26343d89",
      "metadata": {
        "id": "26343d89"
      },
      "source": [
        "### <strong>Q7: Select another Sports Reference web page that follows this pattern and write a program that generates a list of full URLs for that team/organization.\n",
        "\n",
        "A few places to start:\n",
        "- Baseball Reference season web pages have the following URL pattern:\n",
        "  * `https://www.baseball-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.shtml`\n",
        "- Basketball Reference season web pages have a similar pattern for NBA teams:\n",
        "  * `https://www.basketball-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.html`\n",
        "- Basketball Reference uses a slightly different pattern for its WNBA pages:\n",
        "  * `https://www.basketball-reference.com/wnba/teams`, `TEAM ABBREVIATION`, `SEASON`, `.html`\n",
        "- College Basketball Reference pages also follow a pattern: \n",
        "  * `https://www.sports-reference.com/cbb/schools`, `SCHOOL ABBREVIATION`, `SEASON`, `.html`\n",
        "- For Hockey Reference pages: \n",
        "  * `https://www.hockey-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.html`\n",
        "- Football Reference pages follow the same pattern for men's and women's teams:\n",
        "  * `https://fbref.com/en/squads/`, `SQUAD ID`, `SEASON`, `TEAM NAME`\n",
        "- Pro Football Reference pages also have a pattern: \n",
        "  * `https://www.pro-football-reference.com/teams/`, `TEAM ABBREVIATION`, `SEASON`, `.htm`\n",
        "\n",
        "NOTE: You DO NOT need to write a program that scrapes data from these pages for this question. The purpose of this question is to be able to programmatically generate a list of URLs that cover a date range.</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b232bf",
      "metadata": {
        "id": "05b232bf"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c36ea4",
      "metadata": {
        "id": "c1c36ea4"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e72a724",
      "metadata": {
        "id": "7e72a724"
      },
      "source": [
        "### **Q8: Describe the general approach to loading a web page in Python using `pd.read_html()`. What are the basic steps involved in this workflow, thinking about what happens to identify/isolate the specific table you want to work with?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea766802",
      "metadata": {
        "id": "ea766802"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dfad915",
      "metadata": {
        "id": "4dfad915"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8062d2cf",
      "metadata": {
        "id": "8062d2cf"
      },
      "source": [
        "### <strong>Q9: For Q7, you generated a list of Sports Reference URLs covering a time span for a specific team/organization. Select three years and web pages from that list- something early in the time period covered, something in the middle of the time period covered, and something toward the end of the time period covered.\n",
        "    \n",
        "Do these pages have the same pattern in terms of number and order of tables?\n",
        "\n",
        "For one of these pages, what table or tables on these pages would you want to be able to extract and work with?</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4cf0aa5",
      "metadata": {
        "id": "e4cf0aa5"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2494e1f1",
      "metadata": {
        "id": "2494e1f1"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a816b853",
      "metadata": {
        "id": "a816b853"
      },
      "source": [
        "### <strong>Q10: Develop an outline for a Python program that uses `pd.read_html()` to scrape data from one of the web pages you select in Q9.\n",
        "\n",
        "A preliminary workflow:\n",
        "- Use `pd.read_html()` to create a list of DataFrame objects\n",
        "- Identify which DataFrame object in the list is the table you want to work with\n",
        "- Isolate the list element to create a new DataFrame\n",
        "- Write the new DataFrame to a CSV file\n",
        "\n",
        "NOTE: For Q4, you did not need to have working code for all components of this program. Since `pd.read_html()` has an easier learning curve, let's see if we can flesh out more of this program. But if you run into problems, it's okay to focus on the conceptual framework for the web scraping program. Start to build out code where you can, but think about the programming version of outlining a paper.\n",
        "\n",
        "ANOTHER NOTE: For many Sports Reference pages, tables further down the page are buried in HTML comments. These tables will not show up when you use `pd.read_html()`. We can come back to these \"hidden tables\" in the final project, but for now, focus on the tables that do show up when you use `pd.read_html()`.</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ac65ae",
      "metadata": {
        "id": "83ac65ae"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4254bc",
      "metadata": {
        "id": "fa4254bc"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b8f581",
      "metadata": {
        "id": "b5b8f581"
      },
      "source": [
        "### **Q11: What challenges or roadblocks did you face working on Q10? What parts of the program do you understand and/or were able to develop? What parts of the program are less clear?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98975d0b",
      "metadata": {
        "id": "98975d0b"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4205c3",
      "metadata": {
        "id": "db4205c3"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d601517d",
      "metadata": {
        "id": "d601517d"
      },
      "source": [
        "### **Q12: Describe in your own words how program for scraping unstructured text covered in the previous section of the lab works. The full program is also included below. What is happening in the different program components?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99489c2b",
      "metadata": {
        "id": "99489c2b"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c871ae0c",
      "metadata": {
        "id": "c871ae0c"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f3522d8",
      "metadata": {
        "id": "4f3522d8"
      },
      "source": [
        "### <strong>Q13: Select another web page that includes unstructured text. From looking at the public web page, what text do you want to scrape from this web page (i.e. specific sections, multiple paragraphs, etc.)?\n",
        "\n",
        "A few places to start for unstructured text:\n",
        "- [The Observer!](https://ndsmcobserver.com) (or another news publication of your choosing)\n",
        "- [WikiSource](https://en.wikisource.org/wiki/Main_Page)(a library of texts that are not covered by copyright)\n",
        "  * [U.S. Presidential State of the Union Addresses](https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents)\n",
        "  * [U.S. Presidential Inaugural Speeches](https://en.wikisource.org/wiki/Portal:Inaugural_Speeches_by_United_States_Presidents)\n",
        "- [Project Gutenberg](https://www.gutenberg.org) (a library of literary works or texts that are not covered by copyright)</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869dabb1",
      "metadata": {
        "id": "869dabb1"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8aa992",
      "metadata": {
        "id": "4c8aa992"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fc1e7e3",
      "metadata": {
        "id": "0fc1e7e3"
      },
      "source": [
        "### **Q14: Take a look at the HTML for this page. What tags or other HTML components do you see around the section of the page you want to work with? For this question, we're thinking about how we will end up writing a program with `BeautifulSoup` to isolate a section of the web page.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee34d2b",
      "metadata": {
        "id": "3ee34d2b"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2fae89b",
      "metadata": {
        "id": "b2fae89b"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f87ccc",
      "metadata": {
        "id": "75f87ccc"
      },
      "source": [
        "### <strong>Q15: Develop an outline for a Python program that scrapes unstructured text from the web page you selected. \n",
        "\n",
        "A preliminary workflow:\n",
        "- Load URL and create BeautifulSoup object\n",
        "- Isolate section of HTML with your text (either directly or extract from list)\n",
        "- IF NEEDED: Isolate text elements (create list where each element is a section of text)\n",
        "- IF NEEDED: Extract text contents (isolate text from each section/paragraph)\n",
        "- Write text to TXT file\n",
        "\n",
        "NOTE: You do not need to have working code for all components of this program. That's where we're heading with the final project. At this point, we're focusing on the conceptual framework for the web scraping program. Start to build out code where you can, but think about the programming version of outlining a paper.</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09308a15",
      "metadata": {
        "id": "09308a15"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55eba9f",
      "metadata": {
        "id": "b55eba9f"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9456a5ca",
      "metadata": {
        "id": "9456a5ca"
      },
      "source": [
        "### **Q16: What challenges or roadblocks did you face working on Q15? What parts of the program do you understand/feel ready to develop at this point? What parts of the program are less clear?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ee7c28",
      "metadata": {
        "id": "41ee7c28"
      },
      "source": [
        "**Answer**: Your answer here (double click to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ba36e9",
      "metadata": {
        "id": "a3ba36e9"
      },
      "outputs": [],
      "source": [
        "# your codes here (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9750e9d9",
      "metadata": {
        "id": "9750e9d9"
      },
      "source": [
        "# Final Project Next Steps\n",
        "\n",
        "<table>\n",
        " <tr><td>\n",
        "<img src=\"https://elearn.southampton.ac.uk/wp-content/blogs.dir/sites/64/2021/04/PanPan.png\" alt=\"Panopto logo\" width=\"50\"/></td>\n",
        "  <td><a href=\"https://notredame.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=cc961b6a-ef23-4329-a570-ade9005d8411\">Final Project Next Steps</a></td>\n",
        "  </tr>\n",
        "  </table>\n",
        "\n",
        "Our work in this lab is designed to lay the foundation and serve as a springboard for final project work.\n",
        "\n",
        "Specifically, Q4, Q10, and Q13 ask you to develop an outline for web scraping programs using `BeautifulSoup` and `pd.read_html()`.\n",
        "\n",
        "Those questions (and other work for this lab) are the starting place for the final project.\n",
        "\n",
        "The final project for this course involves a web-scraping project written in Python. Specifically, the final project allows you to select a web page (or web pages) and write a Python program (or programs) that downloads select content from that web page as a plain-text file (`CSV`, `TXT`, etc). That content could be paragraphs of text, tables of data, etc. \n",
        "\n",
        "Successful final projects will include two main components:\n",
        "- a well-documented, working Python program written in Jupyter Notebooks\n",
        "- a written reflection (minimum 300 words) that documents how you approached the final project/what you wanted to accomplish via the final project, resources consulted, how you handled challenges you encountered, key takeaways, etc.\n",
        "\n",
        "That reflection can come at the end of the Jupyter Notebook or be embedded throughout the Jupyter Notebook, if you want to approach authoring the notebook as a type of tutorial or \"report\".\n",
        "\n",
        "Expect to spend at least 10 hours working on the final project. That includes brainstorming, meeting with instructor/TAs, in-class work time, etc. If you’re working on a project that is not going to take that much time, think about how to add complexity or take on another smaller scale project.\n",
        "\n",
        "Contact the instructor with questions.\n",
        "\n",
        "**So where to start?**\n",
        "\n",
        "The instructor and TAs are going to move quickly on getting you feedback on this lab. \n",
        "\n",
        "But your work in Q4, Q10, and Q13 should be the starting place for how you think about and approach the final project.\n",
        "\n",
        "Specifically, think about the kinds of web content you were able to scrape for these questions and how you might further develop, refine, or expand your work.\n",
        "- One next step for the final project could be selecting 1-2 of these programs and further developing or refining the code.\n",
        "- Another next step (especially if you were able to develop working programs for these questions) is to think about how you could expand or extend these workflows to multiple web pages or other web data sources.\n",
        "- A third option for next steps would involve thinking expansively about how you could apply the concepts and approaches covered in this lab to a different type of data source/structure. \n",
        "\n",
        "So in the interim, as you're waiting for feedback on this lab, think about where you could go next with expanding and extending your work in this lab, and start to flesh out or develop some of your own ideas about where you put your time and effort as you work on the final project.\n",
        "\n",
        "Contact the instructor with questions."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "python-web-scraping-template.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}